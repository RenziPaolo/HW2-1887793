{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"CDVJcVjPK-jp"},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","from typing import Dict\n","import torch\n","from datasets import load_dataset\n","from transformers import DataCollatorWithPadding\n","from src.base_model import BaseModel\n","from src.KAN_model import KANModel\n","from src.Augement import Augment\n","\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoTokenizer,\n","    Trainer,\n","    TrainingArguments,\n","    set_seed,\n",")\n","\n","\n","### Model Parameters\n","# we will use with Distil-BERT\n","language_model_name = \"microsoft/mdeberta-v3-base\"\n","length = 128\n","\n","### Training Argurments\n","\n","# this GPU should be enough for this task to handle 32 samples per batch\n","batch_size = 32\n","\n","# optim\n","learning_rate = 1e-4\n","weight_decay = 0.001 # we could use e.g. 0.01 in case of very low and very high amount of data for regularization\n","\n","# training\n","epochs = 1\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","\n","set_seed(42)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6887,"status":"ok","timestamp":1716454815232,"user":{"displayName":"Luca Moroni","userId":"02572288400885355292"},"user_tz":-120},"id":"1rFUoPr5DS50","outputId":"da3f8566-37f1-40ed-9964-f6c068c3d460"},"outputs":[],"source":["# load our dataset\n","dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1716454815232,"user":{"displayName":"Luca Moroni","userId":"02572288400885355292"},"user_tz":-120},"id":"8yvWohoaDVKs","outputId":"fcb403f0-544e-438d-9cfa-f0b28c9ca5bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence: {'id': '65960', 'premise': 'Whoopi Goldberg . From 1998 to 2002 , she was co-producer of the television game show Hollywood Squares .', 'hypothesis': 'Whoopi Goldberg co-produced an American dance tournament.', 'label': 'NEUTRAL', 'wsd': {'premise': [{'index': 0, 'text': 'Whoopi', 'pos': 'PROPN', 'lemma': 'Whoopi', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 1, 'text': 'Goldberg', 'pos': 'PROPN', 'lemma': 'Goldberg', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 2, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 3, 'text': 'From', 'pos': 'ADP', 'lemma': 'from', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 4, 'text': '1998', 'pos': 'NUM', 'lemma': '1998', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 5, 'text': 'to', 'pos': 'ADP', 'lemma': 'to', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 6, 'text': '2002', 'pos': 'NUM', 'lemma': '2002', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 7, 'text': ',', 'pos': 'PUNCT', 'lemma': ',', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 8, 'text': 'she', 'pos': 'PRON', 'lemma': 'she', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 9, 'text': 'was', 'pos': 'AUX', 'lemma': 'be', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 10, 'text': 'co', 'pos': 'NOUN', 'lemma': 'co', 'bnSynsetId': 'bn:00020105n', 'wnSynsetOffset': '9957013n', 'nltkSynset': 'conscientious_objector.n.01'}, {'index': 11, 'text': '-', 'pos': 'NOUN', 'lemma': '-', 'bnSynsetId': 'bn:00018345n', 'wnSynsetOffset': '9917593n', 'nltkSynset': 'child.n.01'}, {'index': 12, 'text': 'producer', 'pos': 'NOUN', 'lemma': 'producer', 'bnSynsetId': 'bn:00064581n', 'wnSynsetOffset': '10480018n', 'nltkSynset': 'producer.n.02'}, {'index': 13, 'text': 'of', 'pos': 'ADP', 'lemma': 'of', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 14, 'text': 'the', 'pos': 'DET', 'lemma': 'the', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 15, 'text': 'television', 'pos': 'NOUN', 'lemma': 'television', 'bnSynsetId': 'bn:00076373n', 'wnSynsetOffset': '6277280n', 'nltkSynset': 'television.n.01'}, {'index': 16, 'text': 'game', 'pos': 'NOUN', 'lemma': 'game', 'bnSynsetId': 'bn:00037182n', 'wnSynsetOffset': '430606n', 'nltkSynset': 'game.n.03'}, {'index': 17, 'text': 'show', 'pos': 'VERB', 'lemma': 'show', 'bnSynsetId': 'bn:00086557v', 'wnSynsetOffset': '2148788v', 'nltkSynset': 'show.v.01'}, {'index': 18, 'text': 'Hollywood', 'pos': 'PROPN', 'lemma': 'Hollywood', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 19, 'text': 'Squares', 'pos': 'PROPN', 'lemma': 'Squares', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 20, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}], 'hypothesis': [{'index': 0, 'text': 'Whoopi', 'pos': 'PROPN', 'lemma': 'Whoopi', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 1, 'text': 'Goldberg', 'pos': 'PROPN', 'lemma': 'Goldberg', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 2, 'text': 'co', 'pos': 'VERB', 'lemma': 'co', 'bnSynsetId': 'bn:00099915a', 'wnSynsetOffset': '1326415a', 'nltkSynset': 'co-ed.s.01'}, {'index': 3, 'text': '-', 'pos': 'VERB', 'lemma': '-', 'bnSynsetId': 'bn:00083181v', 'wnSynsetOffset': '2604760v', 'nltkSynset': 'be.v.01'}, {'index': 4, 'text': 'produced', 'pos': 'VERB', 'lemma': 'produce', 'bnSynsetId': 'bn:00084094v', 'wnSynsetOffset': '2157100v', 'nltkSynset': 'produce.v.06'}, {'index': 5, 'text': 'an', 'pos': 'DET', 'lemma': 'an', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}, {'index': 6, 'text': 'American', 'pos': 'ADJ', 'lemma': 'american', 'bnSynsetId': 'bn:00096963a', 'wnSynsetOffset': '2927512a', 'nltkSynset': 'american.a.01'}, {'index': 7, 'text': 'dance', 'pos': 'NOUN', 'lemma': 'dance', 'bnSynsetId': 'bn:00025142n', 'wnSynsetOffset': '7020538n', 'nltkSynset': 'dance.n.01'}, {'index': 8, 'text': 'tournament', 'pos': 'NOUN', 'lemma': 'tournament', 'bnSynsetId': 'bn:00077750n', 'wnSynsetOffset': '7464725n', 'nltkSynset': 'tournament.n.01'}, {'index': 9, 'text': '.', 'pos': 'PUNCT', 'lemma': '.', 'bnSynsetId': 'O', 'wnSynsetOffset': 'O', 'nltkSynset': 'O'}]}, 'srl': {'premise': {'tokens': [{'index': 0, 'rawText': 'Whoopi'}, {'index': 1, 'rawText': 'Goldberg'}, {'index': 2, 'rawText': '.'}, {'index': 3, 'rawText': 'From'}, {'index': 4, 'rawText': '1998'}, {'index': 5, 'rawText': 'to'}, {'index': 6, 'rawText': '2002'}, {'index': 7, 'rawText': ','}, {'index': 8, 'rawText': 'she'}, {'index': 9, 'rawText': 'was'}, {'index': 10, 'rawText': 'co'}, {'index': 11, 'rawText': '-'}, {'index': 12, 'rawText': 'producer'}, {'index': 13, 'rawText': 'of'}, {'index': 14, 'rawText': 'the'}, {'index': 15, 'rawText': 'television'}, {'index': 16, 'rawText': 'game'}, {'index': 17, 'rawText': 'show'}, {'index': 18, 'rawText': 'Hollywood'}, {'index': 19, 'rawText': 'Squares'}, {'index': 20, 'rawText': '.'}], 'annotations': [{'tokenIndex': 9, 'verbatlas': {'frameName': 'COPULA', 'roles': [{'role': 'Time', 'score': 1.0, 'span': [3, 7]}, {'role': 'Theme', 'score': 1.0, 'span': [8, 9]}, {'role': 'Attribute', 'score': 1.0, 'span': [10, 20]}]}, 'englishPropbank': {'frameName': 'be.01', 'roles': [{'role': 'ARGM-TMP', 'score': 1.0, 'span': [3, 7]}, {'role': 'ARG1', 'score': 1.0, 'span': [8, 9]}, {'role': 'ARG2', 'score': 1.0, 'span': [10, 20]}]}}]}, 'hypothesis': {'tokens': [{'index': 0, 'rawText': 'Whoopi'}, {'index': 1, 'rawText': 'Goldberg'}, {'index': 2, 'rawText': 'co'}, {'index': 3, 'rawText': '-'}, {'index': 4, 'rawText': 'produced'}, {'index': 5, 'rawText': 'an'}, {'index': 6, 'rawText': 'American'}, {'index': 7, 'rawText': 'dance'}, {'index': 8, 'rawText': 'tournament'}, {'index': 9, 'rawText': '.'}], 'annotations': [{'tokenIndex': 4, 'verbatlas': {'frameName': 'MOUNT_ASSEMBLE_PRODUCE', 'roles': [{'role': 'Agent', 'score': 1.0, 'span': [0, 2]}, {'role': 'Attribute', 'score': 1.0, 'span': [2, 3]}, {'role': 'Product', 'score': 1.0, 'span': [5, 9]}]}, 'englishPropbank': {'frameName': 'produce.01', 'roles': [{'role': 'ARG0', 'score': 1.0, 'span': [0, 2]}, {'role': 'ARGM-MNR', 'score': 1.0, 'span': [2, 3]}, {'role': 'ARG1', 'score': 1.0, 'span': [5, 9]}]}}]}}}\n"]}],"source":["## Let's see an example...\n","print(f\"Sentence: {dataset['train'][12]}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1716454815232,"user":{"displayName":"Luca Moroni","userId":"02572288400885355292"},"user_tz":-120},"id":"fb4h4PlNDcNl","outputId":"df0748eb-8f14-4b46-bcc3-7ee7ed2dadb9"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n","        num_rows: 51086\n","    })\n","    validation: Dataset({\n","        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n","        num_rows: 2288\n","    })\n","    test: Dataset({\n","        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n","        num_rows: 2287\n","    })\n","})"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["## The structure of the huggingface dataset.\n","dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n","        num_rows: 51086\n","    })\n","    validation: Dataset({\n","        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n","        num_rows: 2288\n","    })\n","    test: Dataset({\n","        features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n","        num_rows: 2287\n","    })\n","})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["augemnter = Augment(dataset, \"test\")\n","augmented_dataset =  augemnter.apply()\n","augmented_dataset"]},{"cell_type":"markdown","metadata":{"id":"5zxaIw4Zljkn"},"source":["### Metric Definition\n","\n","Looking only at cross entropy loss cannot allow us to understand effectively the real capabilities of our NLP model. So let's define a standard method to compute:\n","\n","- **Accuracy** metric\n","- **F1** metric"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"UnQC7RJhJU1l"},"outputs":[],"source":["from datasets import load_metric\n","\n","# Metrics\n","\n","def compute_metrics(eval_pred):\n","   load_accuracy = load_metric(\"accuracy\")\n","   load_f1 = load_metric(\"f1\")\n","\n","   logits, labels = eval_pred\n","   predictions = np.argmax(logits, axis=-1)\n","   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n","   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n","   return {\"accuracy\": accuracy, \"f1\": f1}"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2024,"status":"ok","timestamp":1716454868624,"user":{"displayName":"Luca Moroni","userId":"02572288400885355292"},"user_tz":-120},"id":"BaeWJQhKtZ1H","outputId":"09c9c6b7-17af-48f8-bf0a-a5b577bb3ef4"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["## Initialize the model\n","auto_model = AutoModelForSequenceClassification.from_pretrained(language_model_name,\n","                                                                   ignore_mismatched_sizes=True,\n","                                                                   output_attentions=False, output_hidden_states=False,\n","                                                                   num_labels=3) # number of the classes\n","base_model = BaseModel(device, length, language_model_name)\n","\n","KAN_model = KANModel(device, length, language_model_name)\n","\n","tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","def tokenize_function(examples):\n","    label_map = {\n","        'ENTAILMENT': 0,\n","        'CONTRADICTION': 1,\n","        'NEUTRAL': 2\n","    }\n","    # Map the labels\n","    examples['label'] = [label_map[label] for label in examples['label']]\n","    \n","    # Tokenize the premise and hypothesis\n","    \n","    tokenized = tokenizer(\n","        examples['premise'], \n","        examples['hypothesis'], \n","        truncation=True, \n","        padding='max_length',\n","        max_length=length\n","    )\n","    \n","    # Add tokenized fields to the examples\n","    examples.update(tokenized)\n","    return examples\n","\n","def tokenize_sense_function(examples):\n","    #TODO add word sense\n","    label_map = {\n","        'ENTAILMENT': 0,\n","        'CONTRADICTION': 1,\n","        'NEUTRAL': 2\n","    }\n","    # Map the labels\n","    examples['label'] = [label_map[label] for label in examples['label']]\n","    \n","    # Tokenize the premise and hypothesis\n","    tokenized = tokenizer(\n","        examples['premise'], \n","        examples['hypothesis'], \n","        truncation=True, \n","        padding='max_length',\n","        max_length=length\n","    )\n","    \n","    # Add tokenized fields to the examples\n","    examples.update(tokenized)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["a57be2faae5d45638af3e44a914b51a4","89aafc0136e24694ba6463efb9750dee","b8aa45fb144e435a86512dc9a08850c7","bd361e230d604ea68cf0d6e5811cf727","07953d4ddf8f4fc3b0983adf6b32e2df","2674353de1af46028f3a996d7bd139c6","91cbb2a7875f46f9851c140a73cc3d63","32b8138813054c1dbbc9ffcfae09a956","804240cff4aa45c0a58ffaf623bd47e0","f94aa52c201743d2996a6cd8366faf96","2173a807b5224232bc0ec1293597e077"]},"executionInfo":{"elapsed":636,"status":"ok","timestamp":1716454869253,"user":{"displayName":"Luca Moroni","userId":"02572288400885355292"},"user_tz":-120},"id":"KwXqj8tct-yD","outputId":"90f27219-e611-4039-9ddf-3f096e59ad5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenize the dataset ...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"356acb8af88d4073b5258370cf4b5afa","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/51086 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f6383c3245c847838a2367428dc4970b","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2288 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81f925ef523e4711bbad0e2ab811e299","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2287 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cafc3aab631d407fae982a345dc25d83","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/51086 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9f791bfb0f34570a442452b75d6806d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2288 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c6a8ebd7331411d898b719b2390296c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2287 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bda36fd6f424863b92380571955a359","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/51086 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84f6cdef144d4c6e868b48f27f6dc60e","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/51086 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62057198e92949509983d0079f03c830","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2288 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"030a2d639fe44791b468ce92235822a9","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2287 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Tokenize the dataset put the second phrase as the second parameter to have it concatenated with a <SEP> token\n","print(\"Tokenize the dataset ...\")\n","tokenized_dataset = dataset.map(tokenize_function, batched=True)\n","tokenized_sense_dataset = dataset.map(tokenize_sense_function, batched=True)\n","tokenized_augmented_dataset = augmented_dataset.map(tokenize_function, batched=True)\n","tokenized_augmented_sense_dataset = augmented_dataset.map(tokenize_sense_function, batched=True)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ENTAILMENT\n"]}],"source":["print(dataset['train'][15]['label'])"]},{"cell_type":"markdown","metadata":{"id":"GWmn_wLzooPe"},"source":["## Model Training\n","\n","To train a transformer model you can rely on the **Trainer** class of Huggingface (https://huggingface.co/docs/transformers/main_classes/trainer).\n","\n","The Trainer class allows you to save many lines of code, and makes your code much more readable.\n","\n","To initialize the Trainer class you have to define a **TrainerArguments** object."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ArBIUUy_vDPg"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"training_dir\",                    # output directory [Mandatory]\n","    num_train_epochs=epochs,                      # total number of training epochs\n","    per_device_train_batch_size=batch_size,       # batch size per device during training\n","    warmup_steps=500,                             # number of warmup steps for learning rate scheduler\n","    weight_decay=weight_decay,                    # strength of weight decay\n","    save_strategy=\"no\",\n","    learning_rate=learning_rate                   # learning rate\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"o4rAJRJNucX0"},"outputs":[],"source":["trainer_auto = Trainer(\n","   model=auto_model,\n","   args=training_args,\n","   train_dataset=tokenized_dataset[\"train\"],\n","   eval_dataset=tokenized_dataset[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics\n",")\n","\n","trainer_base = Trainer(\n","   model=base_model,\n","   args=training_args,\n","   train_dataset=tokenized_dataset[\"train\"],\n","   eval_dataset=tokenized_dataset[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics\n",")\n","\n","trainer_KAN = Trainer(\n","   model=KAN_model,\n","   args=training_args,\n","   train_dataset=tokenized_dataset[\"train\"],\n","   eval_dataset=tokenized_dataset[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics\n",")\n","\n","trainer_sense_auto = Trainer(\n","   model=auto_model,\n","   args=training_args,\n","   train_dataset=tokenized_sense_dataset[\"train\"],\n","   eval_dataset=tokenized_sense_dataset[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics\n",")\n","\n","trainer_sense_base = Trainer(\n","   model=base_model,\n","   args=training_args,\n","   train_dataset=tokenized_sense_dataset[\"train\"],\n","   eval_dataset=tokenized_sense_dataset[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics\n",")\n","\n","trainer_sense_KAN = Trainer(\n","   model=KAN_model,\n","   args=training_args,\n","   train_dataset=tokenized_sense_dataset[\"train\"],\n","   eval_dataset=tokenized_sense_dataset[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"executionInfo":{"elapsed":392847,"status":"ok","timestamp":1716455277605,"user":{"displayName":"Luca Moroni","userId":"02572288400885355292"},"user_tz":-120},"id":"8ohc7fZBvjDJ","outputId":"37b419c1-fabf-4624-e6df-3e6e83ab5157"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6e62aaaf7b44bc9b17c8c40bf5d94a1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1597 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Let's Train ...\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer_auto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m trainer_base\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m trainer_KAN\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[1;32mc:\\Users\\User\\Downloads\\informatica\\magistrale\\nlp\\HW2-1887793\\.venv\\lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\User\\Downloads\\informatica\\magistrale\\nlp\\HW2-1887793\\.venv\\lib\\site-packages\\transformers\\trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2222\u001b[0m ):\n\u001b[0;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[1;32mc:\\Users\\User\\Downloads\\informatica\\magistrale\\nlp\\HW2-1887793\\.venv\\lib\\site-packages\\transformers\\trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[1;32mc:\\Users\\User\\Downloads\\informatica\\magistrale\\nlp\\HW2-1887793\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:2125\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2125\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\User\\Downloads\\informatica\\magistrale\\nlp\\HW2-1887793\\.venv\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\User\\Downloads\\informatica\\magistrale\\nlp\\HW2-1887793\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\User\\Downloads\\informatica\\magistrale\\nlp\\HW2-1887793\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Let's Train ...\n","trainer_auto.train()\n","trainer_base.train()\n","trainer_KAN.train()\n","\n","trainer_sense_auto.train()\n","trainer_sense_base.train()\n","trainer_sense_KAN.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":352},"executionInfo":{"elapsed":4377,"status":"ok","timestamp":1716455281959,"user":{"displayName":"Luca Moroni","userId":"02572288400885355292"},"user_tz":-120},"id":"fpvMaurQvktK","outputId":"e46ebbe0-5255-4e67-dc2f-9d95966e6d76"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mL'esecuzione di celle con '.venv (Python 3.9.13)' richiede il pacchetto ipykernel.\n","\u001b[1;31mEseguire il comando seguente per installare 'ipykernel' nell'ambiente Python. \n","\u001b[1;31mComando: 'c:/Users/User/Downloads/informatica/magistrale/nlp/HW2-1887793/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"]}],"source":["# Evaluate the model ...\n","trainer_auto.evaluate()\n","trainer_base.evaluate()\n","trainer_KAN.evaluate()\n","\n","trainer_sense_auto.evaluate()\n","trainer_sense_base.evaluate()\n","trainer_sense_KAN.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mL'esecuzione di celle con '.venv (Python 3.9.13)' richiede il pacchetto ipykernel.\n","\u001b[1;31mEseguire il comando seguente per installare 'ipykernel' nell'ambiente Python. \n","\u001b[1;31mComando: 'c:/Users/User/Downloads/informatica/magistrale/nlp/HW2-1887793/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"]}],"source":["trainer_augmented_auto = Trainer(\n","   model=auto_model,\n","   args=training_args,\n","   train_dataset=tokenized_augmented_datasets[\"train\"],\n","   eval_dataset=tokenized_augmented_datasets[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics,\n",")\n","\n","trainer_augmented_base = Trainer(\n","   model=base_model,\n","   args=training_args,\n","   train_dataset=tokenized_augmented_datasets[\"train\"],\n","   eval_dataset=tokenized_augmented_datasets[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics,\n",")\n","\n","trainer_augmented_KAN = Trainer(\n","   model=KAN_model,\n","   args=training_args,\n","   train_dataset=tokenized_augmented_datasets[\"train\"],\n","   eval_dataset=tokenized_augmented_datasets[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics,\n",")\n","\n","trainer_augmented_sense_auto = Trainer(\n","   model=auto_model,\n","   args=training_args,\n","   train_dataset=tokenized_augmented_sense_datasets[\"train\"],\n","   eval_dataset=tokenized_augmented_sense_datasets[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics,\n",")\n","\n","trainer_augmented_sense_base = Trainer(\n","   model=base_model,\n","   args=training_args,\n","   train_dataset=tokenized_augmented_sense_datasets[\"train\"],\n","   eval_dataset=tokenized_augmented_sense_datasets[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics,\n",")\n","\n","trainer_augmented_sense_KAN = Trainer(\n","   model=KAN_model,\n","   args=training_args,\n","   train_dataset=tokenized_augmented_sense_datasets[\"train\"],\n","   eval_dataset=tokenized_augmented_sense_datasets[\"validation\"],\n","   tokenizer=tokenizer,\n","   data_collator=data_collator,\n","   compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mL'esecuzione di celle con '.venv (Python 3.9.13)' richiede il pacchetto ipykernel.\n","\u001b[1;31mEseguire il comando seguente per installare 'ipykernel' nell'ambiente Python. \n","\u001b[1;31mComando: 'c:/Users/User/Downloads/informatica/magistrale/nlp/HW2-1887793/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"]}],"source":["# Let's Train ...\n","trainer_augmented_auto.train()\n","trainer_augmented_base.train()\n","trainer_augmented_KAN.train()\n","\n","trainer_augmented_sense_auto.train()\n","trainer_augmented_sense_base.train()\n","trainer_augmented_sense_KAN.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mL'esecuzione di celle con '.venv (Python 3.9.13)' richiede il pacchetto ipykernel.\n","\u001b[1;31mEseguire il comando seguente per installare 'ipykernel' nell'ambiente Python. \n","\u001b[1;31mComando: 'c:/Users/User/Downloads/informatica/magistrale/nlp/HW2-1887793/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"]}],"source":["# Evaluate the model ...\n","trainer_augmented_auto.evaluate()\n","trainer_augmented_base.evaluate()\n","trainer_augmented_KAN.evaluate()\n","\n","trainer_augmented_sense_auto.evaluate()\n","trainer_augmented_sense_base.evaluate()\n","trainer_augmented_sense_KAN.evaluate()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1pF6VW7equF-gxKeOnCG6qXhjYna2degw","timestamp":1716546082702},{"file_id":"1abQ4ksp5EU9FA-ibnO4OKV1JlGpxR9SL","timestamp":1682676416498},{"file_id":"1yV8wFHpRRy3y3nnJgK1S4ZNosf7teUrC","timestamp":1653295049649}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"07953d4ddf8f4fc3b0983adf6b32e2df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2173a807b5224232bc0ec1293597e077":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2674353de1af46028f3a996d7bd139c6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32b8138813054c1dbbc9ffcfae09a956":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"804240cff4aa45c0a58ffaf623bd47e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89aafc0136e24694ba6463efb9750dee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2674353de1af46028f3a996d7bd139c6","placeholder":"​","style":"IPY_MODEL_91cbb2a7875f46f9851c140a73cc3d63","value":"Map: 100%"}},"91cbb2a7875f46f9851c140a73cc3d63":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a57be2faae5d45638af3e44a914b51a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89aafc0136e24694ba6463efb9750dee","IPY_MODEL_b8aa45fb144e435a86512dc9a08850c7","IPY_MODEL_bd361e230d604ea68cf0d6e5811cf727"],"layout":"IPY_MODEL_07953d4ddf8f4fc3b0983adf6b32e2df"}},"b8aa45fb144e435a86512dc9a08850c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_32b8138813054c1dbbc9ffcfae09a956","max":1821,"min":0,"orientation":"horizontal","style":"IPY_MODEL_804240cff4aa45c0a58ffaf623bd47e0","value":1821}},"bd361e230d604ea68cf0d6e5811cf727":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f94aa52c201743d2996a6cd8366faf96","placeholder":"​","style":"IPY_MODEL_2173a807b5224232bc0ec1293597e077","value":" 1821/1821 [00:00&lt;00:00, 4770.56 examples/s]"}},"f94aa52c201743d2996a6cd8366faf96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
